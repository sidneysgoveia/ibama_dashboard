import pandas as pd
from supabase import create_client, Client
import time
import os
import sys
import zipfile
import requests
from io import BytesIO
import urllib3
import ssl
from urllib.request import urlopen, Request
from urllib.error import URLError
import subprocess

print("ğŸŒ³ Iniciando processo de upload para o Supabase (VersÃ£o Final)...")

# --- 1. ConfiguraÃ§Ã£o de variÃ¡veis de ambiente ---
def get_env_var(key: str, default: str = None) -> str:
    """ObtÃ©m variÃ¡vel de ambiente com fallback."""
    value = os.getenv(key, default)
    if not value:
        raise ValueError(f"VariÃ¡vel de ambiente {key} nÃ£o encontrada!")
    return value

# ConfiguraÃ§Ãµes
SUPABASE_URL = get_env_var("SUPABASE_URL")
SUPABASE_KEY = get_env_var("SUPABASE_KEY")
IBAMA_ZIP_URL = get_env_var(
    "IBAMA_ZIP_URL", 
    "https://dadosabertos.ibama.gov.br/dados/SIFISC/auto_infracao/auto_infracao/auto_infracao_csv.zip"
)

print(f"ConfiguraÃ§Ãµes carregadas:")
print(f"  - Supabase URL: {SUPABASE_URL[:50]}...")
print(f"  - IBAMA ZIP URL: {IBAMA_ZIP_URL}")

# --- 2. Download ultra robusto ---
def download_with_multiple_methods(url):
    """Tenta mÃºltiplos mÃ©todos para baixar o arquivo."""
    
    # Suprimir avisos de SSL
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    methods = [
        ("requests_no_ssl", lambda: download_with_requests_no_ssl(url)),
        ("urllib_no_ssl", lambda: download_with_urllib_no_ssl(url)),
        ("wget", lambda: download_with_wget(url)),
        ("curl", lambda: download_with_curl(url)),
        ("requests_http", lambda: download_with_requests_http(url)),
    ]
    
    for method_name, method_func in methods:
        print(f"ğŸ”„ Tentando mÃ©todo: {method_name}")
        try:
            content = method_func()
            if content and len(content) > 1000:  # Verifica se baixou algo substancial
                print(f"âœ… Sucesso com {method_name}! Tamanho: {len(content):,} bytes")
                return content
            else:
                print(f"âš ï¸ {method_name}: ConteÃºdo muito pequeno ou vazio")
        except Exception as e:
            print(f"âŒ {method_name} falhou: {str(e)[:100]}...")
    
    raise Exception("âŒ Todos os mÃ©todos de download falharam!")

def download_with_requests_no_ssl(url):
    """Download usando requests sem verificaÃ§Ã£o SSL."""
    session = requests.Session()
    session.verify = False
    response = session.get(url, timeout=300, 
                          headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    response.raise_for_status()
    return response.content

def download_with_urllib_no_ssl(url):
    """Download usando urllib sem verificaÃ§Ã£o SSL."""
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    request = Request(url, headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    with urlopen(request, timeout=300, context=ssl_context) as response:
        return response.read()

def download_with_wget(url):
    """Download usando wget (se disponÃ­vel)."""
    try:
        result = subprocess.run([
            'wget', '--no-check-certificate', '--timeout=300', 
            '--user-agent=Mozilla/5.0 (compatible; IBAMA-Bot/1.0)',
            '-O', '-', url
        ], capture_output=True, check=True, timeout=320)
        return result.stdout
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        raise Exception("wget nÃ£o disponÃ­vel ou falhou")

def download_with_curl(url):
    """Download usando curl (se disponÃ­vel)."""
    try:
        result = subprocess.run([
            'curl', '-k', '--max-time', '300',
            '--user-agent', 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)',
            '-L', url
        ], capture_output=True, check=True, timeout=320)
        return result.stdout
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        raise Exception("curl nÃ£o disponÃ­vel ou falhou")

def download_with_requests_http(url):
    """Tenta a versÃ£o HTTP da URL."""
    http_url = url.replace('https://', 'http://')
    if http_url == url:
        raise Exception("URL jÃ¡ Ã© HTTP")
    
    session = requests.Session()
    response = session.get(http_url, timeout=300,
                          headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    response.raise_for_status()
    return response.content

# --- 3. Processamento inteligente dos dados ---
def download_and_process_data():
    """Download e processa os dados do IBAMA de forma inteligente."""
    print("ğŸ“¥ Baixando dados do IBAMA...")
    
    try:
        # Download ultra robusto
        content = download_with_multiple_methods(IBAMA_ZIP_URL)
        
        print("ğŸ“¦ Processando arquivo ZIP...")
        
        # Extrai o ZIP em memÃ³ria
        with zipfile.ZipFile(BytesIO(content)) as zip_file:
            # Lista arquivos no ZIP
            file_list = zip_file.namelist()
            csv_files = [f for f in file_list if f.endswith('.csv')]
            
            if not csv_files:
                raise ValueError("Nenhum arquivo CSV encontrado no ZIP")
            
            print(f"ğŸ“„ Total de arquivos CSV encontrados: {len(csv_files)}")
            print(f"    Arquivos: {', '.join(csv_files[:5])}{'...' if len(csv_files) > 5 else ''}")
            
            # EstratÃ©gia inteligente: priorizar arquivos 2024-2025, mas ter fallback
            target_years = ['2024', '2025']
            priority_files = [f for f in csv_files if any(year in f for year in target_years)]
            
            if priority_files:
                print(f"ğŸ¯ Arquivos prioritÃ¡rios encontrados (2024-2025): {priority_files}")
                files_to_process = priority_files
            else:
                # Se nÃ£o encontrar arquivos especÃ­ficos, pega os mais recentes
                print("âš ï¸ Arquivos 2024-2025 nÃ£o encontrados. Processando arquivos mais recentes...")
                # Ordena os arquivos pelo nome (que geralmente tem o ano) em ordem decrescente
                sorted_files = sorted(csv_files, reverse=True)
                files_to_process = sorted_files[:5]  # Pega os 5 mais recentes
                print(f"ğŸ“… Processando arquivos mais recentes: {files_to_process}")
            
            # Processa os arquivos selecionados
            all_dataframes = []
            total_records = 0
            
            for csv_file in files_to_process:
                print(f"âš™ï¸ Processando: {csv_file}")
                
                try:
                    df_temp = read_csv_robust(zip_file, csv_file)
                    
                    if df_temp is not None and len(df_temp) > 0:
                        print(f"    âœ… Sucesso: {len(df_temp):,} registros, {len(df_temp.columns)} colunas")
                        all_dataframes.append(df_temp)
                        total_records += len(df_temp)
                    else:
                        print(f"    âš ï¸ Arquivo vazio ou invÃ¡lido: {csv_file}")
                        
                except Exception as e:
                    print(f"    âŒ Erro ao processar {csv_file}: {str(e)[:100]}...")
                    continue
            
            if not all_dataframes:
                raise ValueError("NÃ£o foi possÃ­vel processar nenhum arquivo CSV com dados vÃ¡lidos")
            
            # Combina todos os DataFrames
            print(f"ğŸ”„ Combinando {len(all_dataframes)} arquivos vÃ¡lidos...")
            df = pd.concat(all_dataframes, ignore_index=True, sort=False)
            print(f"ğŸ“Š Dados combinados: {len(df):,} registros, {len(df.columns)} colunas")
            
        # Filtro adicional por data se necessÃ¡rio
        df = apply_date_filter(df)
        
        # Limpeza final
        df = clean_dataframe(df)
        
        if len(df) == 0:
            raise ValueError("Nenhum registro restou apÃ³s filtros e limpeza")
        
        print(f"ğŸ¯ Dados finais prontos: {len(df):,} registros, {len(df.columns)} colunas")
        return df
        
    except Exception as e:
        print(f"âŒ Erro ao baixar/processar dados: {e}")
        raise

def read_csv_robust(zip_file, csv_file):
    """LÃª um arquivo CSV de forma robusta com mÃºltiplas tentativas."""
    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
    separators = [';', ',', '\t']
    
    for encoding in encodings:
        for sep in separators:
            try:
                with zip_file.open(csv_file) as csv_data:
                    df = pd.read_csv(csv_data, encoding=encoding, sep=sep, low_memory=False)
                    
                    # ValidaÃ§Ãµes bÃ¡sicas
                    if len(df.columns) > 5 and len(df) > 0:
                        return df
                        
            except Exception:
                continue
    
    return None

def apply_date_filter(df):
    """Aplica filtro de data se possÃ­vel."""
    if 'DAT_HORA_AUTO_INFRACAO' not in df.columns:
        print("ğŸ“… Coluna de data nÃ£o encontrada, mantendo todos os registros")
        return df
    
    try:
        original_size = len(df)
        
        # Converte a coluna de data
        df['DAT_HORA_AUTO_INFRACAO'] = pd.to_datetime(df['DAT_HORA_AUTO_INFRACAO'], errors='coerce')
        
        # Verifica se conseguimos converter alguma data
        valid_dates = df['DAT_HORA_AUTO_INFRACAO'].notna().sum()
        if valid_dates == 0:
            print("âš ï¸ Nenhuma data vÃ¡lida encontrada, mantendo todos os registros")
            return df
        
        # Filtra pelos anos 2024 e 2025
        df_filtered = df[df['DAT_HORA_AUTO_INFRACAO'].dt.year.isin([2024, 2025])]
        
        if len(df_filtered) > 0:
            print(f"ğŸ“… Filtro de data aplicado (2024-2025): {original_size:,} â†’ {len(df_filtered):,} registros")
            return df_filtered
        else:
            print(f"âš ï¸ Nenhum registro de 2024-2025 encontrado, mantendo todos os {original_size:,} registros")
            return df
            
    except Exception as e:
        print(f"âš ï¸ Erro ao aplicar filtro de data: {e}")
        return df

def clean_dataframe(df):
    """Limpa o DataFrame para upload."""
    # Remove valores NaN
    df = df.fillna('')
    
    # Remove colunas completamente vazias
    df = df.dropna(axis=1, how='all')
    
    # Remove linhas completamente vazias
    df = df.dropna(axis=0, how='all')
    
    return df

# --- 4. ExecuÃ§Ã£o principal ---
try:
    df = download_and_process_data()
    
    if df.empty:
        print("âŒ Nenhum dado foi processado. Encerrando.")
        sys.exit(1)
    
    print(f"âœ… Dados processados com sucesso. Total de {len(df):,} registros.")
    
    # --- 5. Configurar o cliente do Supabase ---
    print("ğŸ”— Conectando ao Supabase...")
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    table_name = "ibama_infracao"
    
    # --- 6. Limpar a tabela existente ---
    print(f"ğŸ§¹ Limpando a tabela '{table_name}' no Supabase...")
    try:
        delete_response = supabase.table(table_name).delete().neq('id', -1).execute()
        print("  âœ… Tabela limpa com sucesso.")
    except Exception as e:
        print(f"âŒ Erro ao limpar a tabela: {e}")
        raise
    
    # --- 7. Upload dos dados em lotes ---
    chunk_size = 1000  # Aumentado para ser mais eficiente
    total_chunks = (len(df) // chunk_size) + 1
    print(f"ğŸš€ Iniciando upload de {len(df):,} registros em {total_chunks} lotes de {chunk_size}...")
    
    successful_uploads = 0
    failed_uploads = 0
    
    for i in range(0, len(df), chunk_size):
        chunk_index = i // chunk_size + 1
        print(f"  ğŸ“¤ Lote {chunk_index}/{total_chunks}...", end=" ")
        
        chunk = df[i:i + chunk_size]
        data_to_insert = chunk.to_dict(orient='records')
        
        try:
            response = supabase.table(table_name).insert(data_to_insert).execute()
            
            if hasattr(response, 'error') and response.error:
                raise Exception(f"Erro da API: {response.error.message}")
            
            successful_uploads += len(data_to_insert)
            print(f"âœ… {len(data_to_insert)} registros")
            
            time.sleep(0.2)  # Pausa otimizada
            
        except Exception as e:
            failed_uploads += len(data_to_insert)
            print(f"âŒ Falha: {str(e)[:50]}...")
            continue
    
    # --- 8. RelatÃ³rio final ---
    print(f"\n{'='*60}")
    print(f"ğŸ“Š RELATÃ“RIO FINAL:")
    print(f"  ğŸ“¥ Total processado: {len(df):,} registros")
    print(f"  âœ… Uploads bem-sucedidos: {successful_uploads:,}")
    print(f"  âŒ Uploads falharam: {failed_uploads:,}")
    print(f"  ğŸ“ˆ Taxa de sucesso: {(successful_uploads/len(df))*100:.1f}%")
    print(f"{'='*60}")
    
    if failed_uploads == 0:
        print("ğŸ‰ Upload para o Supabase concluÃ­do com sucesso!")
        sys.exit(0)
    elif successful_uploads > failed_uploads:
        print("âš ï¸  Upload concluÃ­do com Ãªxito parcial.")
        sys.exit(0)
    else:
        print("âŒ Upload falhou - muitos erros.")
        sys.exit(1)

except Exception as e:
    print(f"ğŸ’¥ Erro crÃ­tico: {e}")
    sys.exit(1)
