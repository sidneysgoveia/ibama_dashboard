import pandas as pd
from supabase import create_client, Client
import time
import os
import sys
import zipfile
import requests
from io import BytesIO
import urllib3
import ssl
from urllib.request import urlopen, Request
import subprocess
import json
import numpy as np
from datetime import datetime
import re

print("ğŸŒ³ Iniciando processo de upload FINAL CORRIGIDO...")

# --- 1. ConfiguraÃ§Ã£o de variÃ¡veis de ambiente ---
def get_env_var(key: str, default: str = None) -> str:
    """ObtÃ©m variÃ¡vel de ambiente com fallback."""
    value = os.getenv(key, default)
    if not value:
        raise ValueError(f"VariÃ¡vel de ambiente {key} nÃ£o encontrada!")
    return value

# ConfiguraÃ§Ãµes
SUPABASE_URL = get_env_var("SUPABASE_URL")
SUPABASE_KEY = get_env_var("SUPABASE_KEY")
IBAMA_ZIP_URL = get_env_var(
    "IBAMA_ZIP_URL", 
    "https://dadosabertos.ibama.gov.br/dados/SIFISC/auto_infracao/auto_infracao/auto_infracao_csv.zip"
)

print(f"ConfiguraÃ§Ãµes carregadas:")
print(f"  - Supabase URL: {SUPABASE_URL[:50]}...")
print(f"  - IBAMA ZIP URL: {IBAMA_ZIP_URL}")

# --- 2. Schema Real do Supabase (baseado no arquivo fornecido) ---
def get_real_supabase_columns(supabase: Client) -> set:
    """ObtÃ©m colunas reais da tabela no Supabase."""
    print("ğŸ” Obtendo schema real do Supabase...")
    
    try:
        # Tenta buscar um registro para ver as colunas reais
        result = supabase.table('ibama_infracao').select('*').limit(1).execute()
        
        if result.data and len(result.data) > 0:
            actual_columns = set(result.data[0].keys())
            print(f"âœ… Schema obtido via SELECT: {len(actual_columns)} colunas")
            
            # Remove colunas do sistema
            system_columns = {'id', 'created_at', 'updated_at'}
            data_columns = actual_columns - system_columns
            
            print(f"ğŸ“‹ Colunas de dados: {len(data_columns)}")
            
            # Log das primeiras colunas para verificaÃ§Ã£o
            sorted_cols = sorted(list(data_columns))
            print("ğŸ“‹ Primeiras 10 colunas encontradas:")
            for i, col in enumerate(sorted_cols[:10], 1):
                print(f"  {i:2d}. {col}")
            
            return data_columns
            
        else:
            print("âš ï¸ Tabela vazia, usando schema conhecido...")
            return get_fallback_schema()
            
    except Exception as e:
        print(f"âŒ Erro ao obter schema: {e}")
        return get_fallback_schema()

def get_fallback_schema() -> set:
    """Schema de fallback baseado na estrutura conhecida."""
    print("ğŸ“‹ Usando schema de fallback...")
    
    # Schema baseado na estrutura tÃ­pica do IBAMA
    known_columns = {
        'SEQ_AUTO_INFRACAO', 'DES_STATUS_FORMULARIO', 'DS_SIT_AUTO_AIE',
        'SIT_CANCELADO', 'NUM_AUTO_INFRACAO', 'SER_AUTO_INFRACAO',
        'CD_ORIGINAL_AUTO_INFRACAO', 'TIPO_AUTO', 'TIPO_MULTA',
        'VAL_AUTO_INFRACAO', 'FUNDAMENTACAO_MULTA', 'PATRIMONIO_APURACAO',
        'GRAVIDADE_INFRACAO', 'CD_NIVEL_GRAVIDADE', 'MOTIVACAO_CONDUTA',
        'EFEITO_MEIO_AMBIENTE', 'EFEITO_SAUDE_PUBLICA', 'PASSIVEL_RECUPERACAO',
        'UNID_ARRECADACAO', 'DES_AUTO_INFRACAO', 'DAT_HORA_AUTO_INFRACAO',
        'FORMA_ENTREGA', 'DAT_CIENCIA_AUTUACAO', 'DT_FATO_INFRACIONAL',
        'DT_INICIO_ATO_INEQUIVOCO', 'DT_FIM_ATO_INEQUIVOCO', 'COD_MUNICIPIO',
        'MUNICIPIO', 'UF', 'NUM_PROCESSO', 'NU_PROCESSO_FORMATADO',
        'COD_INFRACAO', 'DES_INFRACAO', 'TIPO_INFRACAO',
        'CD_RECEITA_AUTO_INFRACAO', 'DES_RECEITA', 'TP_PESSOA_INFRATOR',
        'NUM_PESSOA_INFRATOR', 'NOME_INFRATOR', 'CPF_CNPJ_INFRATOR',
        'QT_AREA', 'INFRACAO_AREA', 'DES_OUTROS_TIPO_AREA',
        'CLASSIFICACAO_AREA', 'DS_FATOR_AJUSTE', 'NUM_LONGITUDE_AUTO',
        'NUM_LATITUDE_AUTO', 'DS_WKT', 'DES_LOCAL_INFRACAO',
        'DS_REFERENCIA_ACAO_FISCALIZATORIA', 'UNIDADE_CONSERVACAO',
        'ID_SICAFI_BIOMAS_ATINGIDOS_INFRACAO', 'DS_BIOMAS_ATINGIDOS',
        'SEQ_NOTIFICACAO', 'SEQ_ACAO_FISCALIZATORIA', 'CD_ACAO_FISCALIZATORIA',
        'UNID_CONTROLE', 'TIPO_ACAO', 'OPERACAO', 'SEQ_ORDEM_FISCALIZACAO',
        'ORDEM_FISCALIZACAO', 'UNID_ORDENADORA', 'SEQ_SOLICITACAO_RECURSO',
        'SOLICITACAO_RECURSO', 'OPERACAO_SOL_RECURSO', 'DT_LANCAMENTO',
        'TP_ULT_ALTERACAO', 'DT_ULT_ALTERACAO', 'JUSTIFICATIVA_ALTERACAO',
        'WKT_GE_AREA_AUTUADA', 'DT_ULT_ALTER_GEOM', 'TP_ORIGEM_GE_AREA_AUTUADA',
        'DS_ERRO_GE_AREA_AUTUADA', 'ST_AUTO_MIGRADO_AIE',
        'DS_ENQUADRAMENTO_ADMINISTRATIVO', 'DS_ENQUADRAMENTO_NAO_ADMINISTRATIVO',
        'DS_ENQUADRAMENTO_COMPLEMENTAR', 'CD_TERMOS_APREENSAO',
        'CD_TERMOS_EMBARGOS', 'TP_ORIGEM_REGISTRO_AUTO',
        'ULTIMA_ATUALIZACAO_RELATORIO'
    }
    
    print(f"ğŸ“‹ Schema de fallback: {len(known_columns)} colunas")
    return known_columns

# --- 3. Download robusto (mantido) ---
def download_with_multiple_methods(url):
    """Tenta mÃºltiplos mÃ©todos para baixar o arquivo."""
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    methods = [
        ("requests_no_ssl", lambda: download_with_requests_no_ssl(url)),
        ("urllib_no_ssl", lambda: download_with_urllib_no_ssl(url)),
        ("wget", lambda: download_with_wget(url)),
        ("curl", lambda: download_with_curl(url)),
        ("requests_http", lambda: download_with_requests_http(url)),
    ]
    
    for method_name, method_func in methods:
        print(f"ğŸ”„ Tentando mÃ©todo: {method_name}")
        try:
            content = method_func()
            if content and len(content) > 1000:
                print(f"âœ… Sucesso com {method_name}! Tamanho: {len(content):,} bytes")
                return content
            else:
                print(f"âš ï¸ {method_name}: ConteÃºdo muito pequeno")
        except Exception as e:
            print(f"âŒ {method_name} falhou: {str(e)[:100]}...")
    
    raise Exception("âŒ Todos os mÃ©todos de download falharam!")

def download_with_requests_no_ssl(url):
    session = requests.Session()
    session.verify = False
    response = session.get(url, timeout=300, 
                          headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    response.raise_for_status()
    return response.content

def download_with_urllib_no_ssl(url):
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    request = Request(url, headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    with urlopen(request, timeout=300, context=ssl_context) as response:
        return response.read()

def download_with_wget(url):
    try:
        result = subprocess.run([
            'wget', '--no-check-certificate', '--timeout=300', 
            '--user-agent=Mozilla/5.0 (compatible; IBAMA-Bot/1.0)',
            '-O', '-', url
        ], capture_output=True, check=True, timeout=320)
        return result.stdout
    except:
        raise Exception("wget nÃ£o disponÃ­vel ou falhou")

def download_with_curl(url):
    try:
        result = subprocess.run([
            'curl', '-k', '--max-time', '300',
            '--user-agent', 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)',
            '-L', url
        ], capture_output=True, check=True, timeout=320)
        return result.stdout
    except:
        raise Exception("curl nÃ£o disponÃ­vel ou falhou")

def download_with_requests_http(url):
    http_url = url.replace('https://', 'http://')
    if http_url == url:
        raise Exception("URL jÃ¡ Ã© HTTP")
    
    session = requests.Session()
    response = session.get(http_url, timeout=300,
                          headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    response.raise_for_status()
    return response.content

# --- 4. Processamento com sincronizaÃ§Ã£o de schema ---
def make_json_serializable(obj):
    """Converte objetos para tipos serializÃ¡veis em JSON."""
    if pd.isna(obj):
        return None
    elif isinstance(obj, (pd.Timestamp, datetime)):
        return obj.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(obj) else None
    elif isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        if np.isnan(obj):
            return None
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, bytes):
        return obj.decode('utf-8', errors='ignore')
    elif isinstance(obj, str):
        cleaned = str(obj).strip()
        return cleaned if cleaned else None
    else:
        return str(obj) if obj is not None else None

def sync_dataframe_with_supabase(df: pd.DataFrame, supabase_columns: set) -> pd.DataFrame:
    """Sincroniza DataFrame com colunas reais do Supabase."""
    print("ğŸ”„ Sincronizando DataFrame com Supabase...")
    
    df_clean = df.copy()
    
    # Verifica correspondÃªncias EXATAS (case-sensitive)
    csv_columns = set(df_clean.columns)
    
    # Encontra correspondÃªncias exatas
    exact_matches = csv_columns & supabase_columns
    
    # Encontra colunas do CSV que nÃ£o existem no Supabase
    extra_columns = csv_columns - supabase_columns
    
    # Encontra colunas do Supabase que nÃ£o estÃ£o no CSV
    missing_columns = supabase_columns - csv_columns
    
    print(f"  ğŸ“Š Colunas no CSV: {len(csv_columns)}")
    print(f"  ğŸ›ï¸ Colunas no Supabase: {len(supabase_columns)}")
    print(f"  âœ… CorrespondÃªncias exatas: {len(exact_matches)}")
    print(f"  âŒ Extras no CSV: {len(extra_columns)}")
    print(f"  âš ï¸ Ausentes no CSV: {len(missing_columns)}")
    
    # Verifica se CD_RECEITA_AUTO_INFRACAO estÃ¡ sendo encontrada
    if 'CD_RECEITA_AUTO_INFRACAO' in exact_matches:
        print(f"  âœ… CD_RECEITA_AUTO_INFRACAO: correspondÃªncia exata encontrada")
    elif 'CD_RECEITA_AUTO_INFRACAO' in extra_columns:
        print(f"  âŒ CD_RECEITA_AUTO_INFRACAO: existe no CSV mas NÃƒO no Supabase")
    elif 'CD_RECEITA_AUTO_INFRACAO' in missing_columns:
        print(f"  âŒ CD_RECEITA_AUTO_INFRACAO: existe no Supabase mas NÃƒO no CSV")
    else:
        print(f"  â“ CD_RECEITA_AUTO_INFRACAO: nÃ£o encontrada em lugar nenhum")
    
    # Log de colunas problemÃ¡ticas (primeiras 10)
    if extra_columns:
        print(f"\nğŸ—‘ï¸ Colunas que serÃ£o removidas (primeiras 10):")
        for i, col in enumerate(sorted(extra_columns)[:10], 1):
            print(f"  {i:2d}. {col}")
        if len(extra_columns) > 10:
            print(f"  ... e mais {len(extra_columns) - 10} colunas")
    
    # MantÃ©m apenas colunas que existem no Supabase
    columns_to_keep = list(exact_matches)
    df_synced = df_clean[columns_to_keep].copy()
    
    print(f"\nâœ… DataFrame sincronizado: {len(df_synced)} registros, {len(df_synced.columns)} colunas")
    
    # Processa cada coluna para garantir compatibilidade
    for col in df_synced.columns:
        df_synced[col] = df_synced[col].apply(make_json_serializable)
        
        # Tratamento especial para colunas numÃ©ricas conhecidas
        numeric_columns = {
            'CD_RECEITA_AUTO_INFRACAO', 'SEQ_AUTO_INFRACAO', 'COD_MUNICIPIO', 
            'COD_INFRACAO', 'NUM_PROCESSO', 'NUM_PESSOA_INFRATOR',
            'SEQ_NOTIFICACAO', 'SEQ_ACAO_FISCALIZATORIA', 'SEQ_ORDEM_FISCALIZACAO',
            'SEQ_SOLICITACAO_RECURSO', 'SOLICITACAO_RECURSO'
        }
        
        if col in numeric_columns:
            try:
                df_synced[col] = pd.to_numeric(df_synced[col], errors='coerce')
                df_synced[col] = df_synced[col].where(pd.notna(df_synced[col]), None)
            except:
                print(f"    âš ï¸ Erro na conversÃ£o numÃ©rica de {col}")
    
    # Valida colunas essenciais
    essential_columns = {'NUM_AUTO_INFRACAO', 'UF', 'TIPO_INFRACAO'}
    missing_essential = essential_columns - set(df_synced.columns)
    
    if missing_essential:
        raise ValueError(f"âŒ Colunas essenciais ausentes: {missing_essential}")
    else:
        print(f"âœ… Todas as colunas essenciais presentes")
    
    return df_synced

def read_csv_robust(zip_file, csv_file):
    """LÃª um arquivo CSV de forma robusta."""
    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
    separators = [';', ',', '\t']
    
    for encoding in encodings:
        for sep in separators:
            try:
                with zip_file.open(csv_file) as csv_data:
                    df = pd.read_csv(csv_data, encoding=encoding, sep=sep, low_memory=False)
                    if len(df.columns) > 5 and len(df) > 0:
                        return df
            except Exception:
                continue
    return None

# --- 5. Processamento principal ---
def download_and_process_data(supabase_columns: set):
    """Download e processa os dados do IBAMA."""
    print("ğŸ“¥ Baixando dados do IBAMA...")
    
    try:
        # Download
        content = download_with_multiple_methods(IBAMA_ZIP_URL)
        
        print("ğŸ“¦ Processando arquivo ZIP...")
        
        # Processa ZIP
        with zipfile.ZipFile(BytesIO(content)) as zip_file:
            file_list = zip_file.namelist()
            csv_files = [f for f in file_list if f.endswith('.csv')]
            
            if not csv_files:
                raise ValueError("Nenhum arquivo CSV encontrado no ZIP")
            
            print(f"ğŸ“„ Total de arquivos CSV: {len(csv_files)}")
            
            # Busca arquivos 2024-2025
            target_files = [f for f in csv_files if any(year in f for year in ['2024', '2025'])]
            
            if target_files:
                print(f"ğŸ¯ Arquivos encontrados (2024-2025): {target_files}")
                files_to_process = target_files
            else:
                print("âš ï¸ Arquivos 2024-2025 nÃ£o encontrados. Usando os mais recentes...")
                files_to_process = sorted(csv_files, reverse=True)[:5]
            
            # Processa arquivos
            all_dataframes = []
            
            for csv_file in files_to_process:
                print(f"âš™ï¸ Processando: {csv_file}")
                
                try:
                    df_temp = read_csv_robust(zip_file, csv_file)
                    
                    if df_temp is not None and len(df_temp) > 0:
                        print(f"    âœ… {len(df_temp):,} registros, {len(df_temp.columns)} colunas")
                        all_dataframes.append(df_temp)
                    else:
                        print(f"    âš ï¸ Arquivo vazio: {csv_file}")
                        
                except Exception as e:
                    print(f"    âŒ Erro: {str(e)[:100]}...")
                    continue
            
            if not all_dataframes:
                raise ValueError("Nenhum arquivo vÃ¡lido processado")
            
            # Combina DataFrames
            print(f"ğŸ”„ Combinando {len(all_dataframes)} arquivos...")
            df = pd.concat(all_dataframes, ignore_index=True, sort=False)
            print(f"ğŸ“Š Dados combinados: {len(df):,} registros")
        
        # Sincroniza com Supabase
        df_synced = sync_dataframe_with_supabase(df, supabase_columns)
        
        return df_synced
        
    except Exception as e:
        print(f"âŒ Erro no processamento: {e}")
        raise

# --- 6. Upload otimizado ---
def safe_upload_batch(supabase: Client, table_name: str, data_batch: list, batch_index: int):
    """Upload seguro de um lote com debug detalhado."""
    try:
        # Limpa registros
        cleaned_batch = []
        for record in data_batch:
            cleaned_record = {}
            for key, value in record.items():
                if key and key.strip():
                    if pd.isna(value):
                        cleaned_record[key] = None
                    elif isinstance(value, (list, dict)):
                        cleaned_record[key] = json.dumps(value) if value else None
                    else:
                        cleaned_record[key] = value
            cleaned_batch.append(cleaned_record)
        
        # Upload
        response = supabase.table(table_name).insert(cleaned_batch).execute()
        
        if hasattr(response, 'error') and response.error:
            raise Exception(f"Erro da API: {response.error}")
        
        return True, len(cleaned_batch)
        
    except Exception as e:
        error_msg = str(e)
        
        # Debug detalhado para primeiros erros
        if batch_index <= 5:
            print(f"\nğŸ” DEBUG - Lote {batch_index}:")
            print(f"  Tamanho: {len(data_batch)} registros")
            print(f"  Colunas: {len(data_batch[0].keys())} colunas")
            print(f"  Erro: {error_msg[:400]}...")
            
            # Mostra as primeiras 10 colunas do registro problemÃ¡tico
            if data_batch:
                first_record_keys = list(data_batch[0].keys())[:10]
                print(f"  Primeiras colunas: {first_record_keys}")
            
            # Tenta identificar coluna especÃ­fica no erro
            if "could not find" in error_msg.lower():
                import re
                match = re.search(r"could not find the '([^']+)'", error_msg, re.IGNORECASE)
                if match:
                    problematic_column = match.group(1)
                    print(f"  ğŸš¨ Coluna problemÃ¡tica: {problematic_column}")
                    
                    # Verifica se a coluna existe nos dados
                    if data_batch and problematic_column in data_batch[0]:
                        print(f"  ğŸ“‹ Coluna EXISTS nos dados - problema pode ser no schema do Supabase")
                    else:
                        print(f"  âŒ Coluna NOT EXISTS nos dados")
        
        return False, error_msg

# --- 7. ExecuÃ§Ã£o principal ---
try:
    # Conecta ao Supabase
    print("ğŸ”— Conectando ao Supabase...")
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    table_name = "ibama_infracao"
    
    # ObtÃ©m schema real do Supabase
    supabase_columns = get_real_supabase_columns(supabase)
    print(f"âœ… Schema do Supabase carregado: {len(supabase_columns)} colunas")
    
    # Download e processamento
    df = download_and_process_data(supabase_columns)
    
    if df.empty:
        print("âŒ Nenhum dado processado apÃ³s sincronizaÃ§Ã£o.")
        sys.exit(1)
    
    print(f"âœ… Dados sincronizados: {len(df):,} registros, {len(df.columns)} colunas")
    
    # Teste de serializaÃ§Ã£o final
    print("ğŸ” Teste final de compatibilidade...")
    try:
        test_record = df.iloc[0].to_dict()
        json.dumps(test_record, default=str)
        print("  âœ… SerializaÃ§Ã£o OK")
    except Exception as e:
        print(f"  âŒ Erro na serializaÃ§Ã£o: {e}")
        raise
    
    # Limpa tabela
    print(f"ğŸ§¹ Limpando tabela '{table_name}'...")
    try:
        delete_result = supabase.table(table_name).delete().neq('id', -1).execute()
        print("  âœ… Tabela limpa")
    except Exception as e:
        print(f"  âš ï¸ Aviso na limpeza: {e}")
        print("  âš ï¸ Continuando com upload...")
    
    # Upload final em lotes pequenos
    chunk_size = 25  # Lotes bem pequenos para debug completo
    total_chunks = (len(df) // chunk_size) + 1
    print(f"ğŸš€ Upload FINAL: {len(df):,} registros em {total_chunks} lotes de {chunk_size}")
    
    successful_uploads = 0
    failed_uploads = 0
    errors_log = []
    
    for i in range(0, len(df), chunk_size):
        chunk_index = i // chunk_size + 1
        print(f"  ğŸ“¤ Lote {chunk_index}/{total_chunks}...", end=" ")
        
        chunk = df[i:i + chunk_size]
        data_to_insert = chunk.to_dict(orient='records')
        
        success, result = safe_upload_batch(supabase, table_name, data_to_insert, chunk_index)
        
        if success:
            successful_uploads += result
            print(f"âœ… {result} registros")
        else:
            failed_uploads += len(data_to_insert)
            errors_log.append(f"Lote {chunk_index}: {result}")
            print(f"âŒ Erro")
            
            # Para apÃ³s alguns erros para anÃ¡lise detalhada
            if chunk_index >= 10:
                print(f"\nğŸ›‘ Parando apÃ³s {chunk_index} tentativas para anÃ¡lise completa")
                break
        
        time.sleep(0.05)  # Pausa mÃ­nima
    
    # RelatÃ³rio final detalhado
    print(f"\n{'='*70}")
    print(f"ğŸ“Š RELATÃ“RIO FINAL DO UPLOAD:")
    print(f"  ğŸ“¥ Total de registros: {len(df):,}")
    print(f"  âœ… Upload bem-sucedido: {successful_uploads:,}")
    print(f"  âŒ Upload com falha: {failed_uploads:,}")
    print(f"  ğŸ¯ Colunas sincronizadas: {len(df.columns)}")
    
    if len(df) > 0:
        success_rate = (successful_uploads / len(df)) * 100
        print(f"  ğŸ“ˆ Taxa de sucesso: {success_rate:.1f}%")
    
    # Status final baseado na taxa de sucesso
    if successful_uploads > len(df) * 0.9:  # 90% ou mais
        print(f"\nğŸ‰ UPLOAD CONCLUÃDO COM SUCESSO!")
        print(f"ğŸ‰ {successful_uploads:,} registros carregados no Supabase")
        status_code = 0
        
    elif successful_uploads > len(df) * 0.5:  # 50% ou mais
        print(f"\nâš ï¸ UPLOAD PARCIALMENTE CONCLUÃDO")
        print(f"âš ï¸ {successful_uploads:,} de {len(df):,} registros carregados")
        status_code = 0
        
    elif successful_uploads > 0:
        print(f"\nâš ï¸ UPLOAD COM PROBLEMAS")
        print(f"âš ï¸ Apenas {successful_uploads:,} registros foram carregados")
        status_code = 1
        
    else:
        print(f"\nâŒ UPLOAD FALHOU COMPLETAMENTE")
        print(f"âŒ Nenhum registro foi carregado no Supabase")
        status_code = 1
    
    # Log de erros (primeiros 5)
    if errors_log:
        print(f"\nğŸš¨ PRIMEIROS ERROS ENCONTRADOS:")
        for i, error in enumerate(errors_log[:5], 1):
            print(f"  {i}. {error}")
        
        if len(errors_log) > 5:
            print(f"  ... e mais {len(errors_log) - 5} erros")
    
    # Resumo da sincronizaÃ§Ã£o de schema
    print(f"\nğŸ“‹ RESUMO DA SINCRONIZAÃ‡ÃƒO:")
    print(f"  ğŸ›ï¸ Colunas no Supabase: {len(supabase_columns)}")
    print(f"  ğŸ“„ Colunas no CSV: informaÃ§Ã£o processada")
    print(f"  âœ… Colunas sincronizadas: {len(df.columns)}")
    
    # Verifica se a coluna problemÃ¡tica foi resolvida
    if 'CD_RECEITA_AUTO_INFRACAO' in df.columns:
        print(f"  âœ… CD_RECEITA_AUTO_INFRACAO: incluÃ­da no upload")
    else:
        print(f"  âŒ CD_RECEITA_AUTO_INFRACAO: removida (nÃ£o existe no Supabase)")
    
    print(f"{'='*70}")
    
    # InstruÃ§Ãµes finais
    if status_code == 0:
        print(f"\nğŸ’¡ PRÃ“XIMOS PASSOS:")
        print(f"  1. Verifique os dados no Supabase Dashboard")
        print(f"  2. Teste o aplicativo IBAMA Dashboard")
        print(f"  3. Execute consultas de validaÃ§Ã£o se necessÃ¡rio")
        
        if failed_uploads > 0:
            print(f"\nğŸ’¡ PARA RESOLVER REGISTROS COM FALHA:")
            print(f"  1. Analise os erros detalhados acima")
            print(f"  2. Execute novamente com chunk_size menor")
            print(f"  3. Considere upload dos registros faltantes separadamente")
    else:
        print(f"\nğŸ’¡ PARA RESOLVER OS PROBLEMAS:")
        print(f"  1. Analise os erros detalhados acima")
        print(f"  2. Verifique se a tabela 'ibama_infracao' existe no Supabase")
        print(f"  3. Confirme se o schema da tabela estÃ¡ correto")
        print(f"  4. Verifique permissÃµes de escrita na tabela")
        print(f"  5. Execute o script check_schema.py para diagnÃ³stico")
    
    sys.exit(status_code)

except KeyboardInterrupt:
    print(f"\nâ¸ï¸ Upload interrompido pelo usuÃ¡rio")
    sys.exit(1)
    
except Exception as e:
    print(f"\nğŸ’¥ ERRO CRÃTICO:")
    print(f"ğŸ’¥ {str(e)}")
    
    # Debug adicional
    print(f"\nğŸ” INFORMAÃ‡Ã•ES DE DEBUG:")
    print(f"  ğŸ“¡ Supabase URL: {SUPABASE_URL[:50]}...")
    print(f"  ğŸ”‘ Supabase Key: {'***' if SUPABASE_KEY else 'NÃƒO DEFINIDA'}")
    print(f"  ğŸ“Š Tabela: {table_name}")
    
    # Stack trace para debug tÃ©cnico
    print(f"\nğŸ“‹ STACK TRACE COMPLETO:")
    import traceback
    traceback.print_exc()
    
    print(f"\nğŸ’¡ SUGESTÃ•ES PARA RESOLVER:")
    print(f"  1. Verifique se as variÃ¡veis de ambiente estÃ£o corretas")
    print(f"  2. Confirme se a tabela 'ibama_infracao' existe no Supabase")
    print(f"  3. Teste a conexÃ£o com o Supabase manualmente")
    print(f"  4. Execute check_schema.py para diagnÃ³stico detalhado")
    
    sys.exit(1)
